{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spatial reproducibility between the training and the test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import h5py\n",
    "import glob\n",
    "import dypac\n",
    "import pickle\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import nibabel as nb\n",
    "from nilearn import plotting\n",
    "from scipy.stats import pearsonr\n",
    "from nilearn.input_data import NiftiMasker\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from nilearn import plotting\n",
    "from nilearn.masking import unmask\n",
    "from scipy.optimize import linear_sum_assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Match the components from teh first set (training set) with the components of the second set (test set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Dypac components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bg_path = 'path to mask'\n",
    "\n",
    "main_path = 'main folder'\n",
    "output_folder_within_rep = os.path.join(main_path, 'within-subject-reproducibility')\n",
    "output_folder_btw_rep = os.path.join(main_path, 'between-subject-reproducibility')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_subject = ['sub-01','sub-02']\n",
    "\n",
    "list_clusters = [20]\n",
    "list_states = [150]\n",
    "list_fwhm = [8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_fwhm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_states(maps1, maps2):\n",
    "    \"\"\"\n",
    "    Match states computed based on two independent sets of data\n",
    "    based on the Hungarian matching algorithm\n",
    "    :param maps1: Dask array, shape (n_states, n_voxels)\n",
    "            n_states's dynamic parcel for each state for the set 1 of sessions\n",
    "    :param maps2: Dask array, shape (n_states, n_voxels)\n",
    "            n_states's dynamic parcel for each state for the set 2 of sessions\n",
    "    :return: float\n",
    "        the average correlation related to this medoid\n",
    "    \"\"\"\n",
    "    n_states = maps1.shape[0]\n",
    "\n",
    "    states_cost_mtx = np.zeros((n_states, n_states))\n",
    "    states_corr_mtx = np.zeros((n_states, n_states))\n",
    "\n",
    "    n_par_state1 = 0\n",
    "    \n",
    "    for map1_idx in range(n_states):\n",
    "        \n",
    "        map1 = maps1[map1_idx, :]\n",
    "        \n",
    "        n_voxels1 = map1.shape[0]\n",
    "        n_par_state2 = 0\n",
    "        for map2_idx in range(n_states):\n",
    "            \n",
    "            map2 = maps2[map2_idx, :]\n",
    "            \n",
    "            corr, _ = pearsonr(map1, map2) \n",
    "            \n",
    "            if np.isnan(corr):\n",
    "                continue\n",
    "            \n",
    "            states_cost_mtx[n_par_state1, n_par_state2] = (1 - corr)\n",
    "            \n",
    "            # Correlation between pairwise states\n",
    "            states_corr_mtx[n_par_state1, n_par_state2] = corr\n",
    "\n",
    "            n_par_state2 += 1\n",
    "        n_par_state1 += 1\n",
    "\n",
    "    row_ind, col_ind = linear_sum_assignment(states_cost_mtx)\n",
    "\n",
    "    l_scores_states = []\n",
    "    l_tuple_matched_states = []\n",
    "    for n_state in range(n_states):\n",
    "        l_scores_states.append(states_corr_mtx[row_ind[n_state], col_ind[n_state]])\n",
    "        l_tuple_matched_states.append(tuple( (row_ind[n_state], col_ind[n_state])))\n",
    "\n",
    "    return l_tuple_matched_states, l_scores_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Within-subject level spatial reproducibility "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for subject in list_subject:\n",
    "    \n",
    "    out_filename_within = os.path.join(output_folder, 'within-'+subject+'-reproducibility-fwhm-5.h5')\n",
    "\n",
    "    h5_file = h5py.File(out_filename_within, 'a')\n",
    "    \n",
    "    h5_subject = h5_file.create_group(subject)\n",
    "    \n",
    "    ###  Load mask for each subject\n",
    "    bg_img = os.path.join(bg_path, subject + '_space-MNI152NLin2009cAsym_label-GM_probseg.nii.gz')\n",
    "    \n",
    "    bg_img = nb.load(bg_img)\n",
    "\n",
    "    for fwhm in list_fwhm:\n",
    "        \n",
    "        h5_fwhm = h5_subject.create_group(str(fwhm))\n",
    "        \n",
    "        for cluster in list_clusters:\n",
    "            \n",
    "            h5_cluster = h5_fwhm.create_group(str(cluster))\n",
    "\n",
    "            for states in list_states:\n",
    "                \n",
    "                \n",
    "                h5_states = h5_cluster.create_group(str(states))\n",
    "\n",
    "                ## TRAINING DATA COMPONENTS\n",
    "                file_ext = subject+ '_dataset-friends_tasks-s01even_cluster-'+str(cluster)+'_states-'+str(states)+'_batches-1_reps-100_fwhm-'+str(fwhm)+'.pickle'\n",
    "\n",
    "                files_td = os.path.join(main_path, 'models-embeddings-friends', file_ext) \n",
    "\n",
    "                list_files_td = glob.glob(files_td, recursive=True)\n",
    "                \n",
    "                print(files_td)\n",
    "\n",
    "                if len(list_files_td) > 0:\n",
    "                    \n",
    "                    if os.path.exists(list_files_td[0]):\n",
    "\n",
    "                        with (open(list_files_td[0], \"rb\")) as openfile:\n",
    "\n",
    "                            model_td = pickle.load(openfile)\n",
    "\n",
    "                        mask_binary = bg_img.get_data() > 0\n",
    "\n",
    "                        mask_binary = mask_binary.astype(int)\n",
    "\n",
    "                        n_voxels = len(np.where(mask_binary==1)[0])\n",
    "\n",
    "                        mask_img = nb.Nifti1Image(mask_binary, bg_img.affine)\n",
    "\n",
    "                        del mask_binary\n",
    "\n",
    "                        nifti_masker = NiftiMasker(mask_img=mask_img, standardize=False,\n",
    "                                               smoothing_fwhm=None, detrend=False,\n",
    "                                               memory=\"nilearn_cache\", memory_level=1)\n",
    "\n",
    "                        del mask_img\n",
    "                        gc.collect()\n",
    "\n",
    "                        nifti_masker.fit(bg_img)\n",
    "\n",
    "                        stable_cp_td = np.zeros((model_td.components_.shape[0], n_voxels))\n",
    "\n",
    "                        print('shape: ', model_td.components_.shape[0])\n",
    "\n",
    "                        for idx in range(model_td.components_.shape[0]):\n",
    "\n",
    "                            img = model_td.masker_.inverse_transform(model_td.components_[idx,:].todense())\n",
    "\n",
    "                            stable_cp_td[idx, :] = nifti_masker.transform(img)\n",
    "\n",
    "                        del img   \n",
    "                        del model_td\n",
    "                        gc.collect()\n",
    "\n",
    "                        file_ext = subject + '_dataset-friends_tasks-s01odd_cluster-'+str(cluster)+'_states-'+str(states)+'_batches-1_reps-100_fwhm-'+str(fwhm)+'.pickle'\n",
    "\n",
    "                        files_tstd = os.path.join(main_path, 'models-embeddings-friends', file_ext) \n",
    "\n",
    "                        list_files_tstd= glob.glob(files_tstd, recursive=True)\n",
    "\n",
    "                        with (open(list_files_tstd[0], \"rb\")) as openfile:\n",
    "\n",
    "                            model_tstd = pickle.load(openfile)\n",
    "\n",
    "                        stable_cp_tstd = np.zeros((model_tstd.components_.shape[0], n_voxels))\n",
    "\n",
    "                        for idx in range(model_tstd.components_.shape[0]):\n",
    "\n",
    "                            img = model_tstd.masker_.inverse_transform(model_tstd.components_[idx,:].todense())\n",
    "\n",
    "                            stable_cp_tstd[idx, :] = nifti_masker.transform(img)\n",
    "\n",
    "                        del img\n",
    "                        del nifti_masker\n",
    "                        del model_tstd\n",
    "                        gc.collect()\n",
    "\n",
    "                        ## MATCHING COMPONENTS AT THE WITHIN-SUBJECT LEVEL \n",
    "                        tuple_matched_states, scores_states = match_states(stable_cp_td, stable_cp_tstd)\n",
    "\n",
    "                        del stable_cp_td\n",
    "                        del stable_cp_tstd\n",
    "                        gc.collect()\n",
    "\n",
    "                        print(scores_states)\n",
    "\n",
    "                    h5_states.create_dataset('Matched-state-maps', data=np.asarray(tuple_matched_states))\n",
    "\n",
    "                    h5_states.create_dataset('States-reproducibility', data=np.asarray(scores_states))\n",
    "\n",
    "                    del scores_states\n",
    "                    del tuple_matched_states\n",
    "                    gc.collect()\n",
    "                    \n",
    "                \n",
    "\n",
    "h5_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_idx_scores = np.where(scores_states>=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Between-subject spatial reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sub_id1 in range(len(list_subject)):\n",
    "    \n",
    "    for sub_id2 in range(sub_id1+1, len(list_subject)):\n",
    "        \n",
    "        subject1 = list_subject[sub_id1]\n",
    "        \n",
    "        subject2 = list_subject[sub_id2]\n",
    "        \n",
    "        print(subject1 +'-'+subject2)    \n",
    "        \n",
    "        for fwhm in list_fwhm:\n",
    "                \n",
    "                out_filename_between = os.path.join(output_folder_btw_rep, \n",
    "                            'draft-between-subjects-reproducibility', 'between-subjects-reproducibility' +subject1+'-'+subject2+'-reproducibility-fwhm'+str(fwhm)+'.h5')\n",
    "\n",
    "                h5_file = h5py.File(out_filename_between, 'a')\n",
    "                \n",
    "                h5_subject = h5_file.create_group(subject1 +'-'+subject2)\n",
    "                \n",
    "                h5_fwhm = h5_subject.create_group(str(fwhm))\n",
    "\n",
    "                for cluster in list_clusters:\n",
    "                    \n",
    "                        #try:\n",
    "\n",
    "                            h5_cluster = h5_fwhm.create_group(str(cluster))\n",
    "\n",
    "                            for states in list_states:\n",
    "\n",
    "                                h5_states = h5_cluster.create_group(str(states))\n",
    "\n",
    "                                ###  Load mask for each subject\n",
    "                                bg_img1 = os.path.join(bg_path, subject1 + '_space-MNI152NLin2009cAsym_label-GM_probseg.nii.gz')\n",
    "                                bg_img2 = os.path.join(bg_path, subject2 + '_space-MNI152NLin2009cAsym_label-GM_probseg.nii.gz')\n",
    "\n",
    "                                ## TRAINING DATA COMPONENTS\n",
    "                                file_ext1 = subject1 + '_dataset-friends_tasks-s01odd_cluster-'+str(cluster)+'_states-'+str(states)+'_batches-1_reps-100_fwhm-'+str(fwhm)+'.pickle'\n",
    "\n",
    "                                files_td1 = os.path.join(main_path, 'models-embeddings-friends', file_ext1) \n",
    "\n",
    "                                print(files_td1)\n",
    "                                \n",
    "                                list_files_td1= glob.glob(files_td1, recursive=True)\n",
    "                                \n",
    "                                print(list_files_td1)\n",
    "\n",
    "                                with (open(list_files_td1[0], \"rb\")) as openfile:\n",
    "\n",
    "                                    model_td = pickle.load(openfile)\n",
    "\n",
    "                                bg_img1 = nb.load(bg_img1)\n",
    "                                bg_img2 = nb.load(bg_img2)\n",
    "\n",
    "                                mask_binary1 = bg_img1.get_data() > 0\n",
    "                                mask_binary2 = bg_img2.get_data() > 0\n",
    "\n",
    "                                mask_binary = mask_binary1 == mask_binary2\n",
    "\n",
    "                                mask_binary = mask_binary.astype(int)\n",
    "\n",
    "                                bg_img = nb.Nifti1Image(mask_binary, bg_img1.affine)\n",
    "\n",
    "                                n_voxels = len(np.where(mask_binary==1)[0])\n",
    "\n",
    "                                print('n_voxels')\n",
    "                                print(n_voxels)\n",
    "\n",
    "                                mask_img = nb.Nifti1Image(mask_binary, bg_img.affine)\n",
    "\n",
    "                                nifti_masker = NiftiMasker(mask_img=mask_img, standardize=False,\n",
    "                                                       smoothing_fwhm=None, detrend=False,\n",
    "                                                       memory=\"nilearn_cache\", memory_level=1)\n",
    "\n",
    "                                nifti_masker.fit(bg_img)\n",
    "\n",
    "                                stable_cp_td = np.zeros((model_td.components_.shape[0], n_voxels))\n",
    "\n",
    "                                for idx in range(model_td.components_.shape[0]):\n",
    "\n",
    "                                    img =  model_td.masker_.inverse_transform(model_td.components_[idx,:].todense())\n",
    "\n",
    "                                    stable_cp_td[idx, :] = nifti_masker.transform(img)\n",
    "\n",
    "                                gc.collect()\n",
    "\n",
    "                                ## TEST DATA COMPONENTS\n",
    "                                file_ext2 = subject2 + '_dataset-friends_tasks-s01even_cluster-'+str(cluster)+'_states-'+str(states)+'_batches-1_reps-100_fwhm-'+str(fwhm)+'.pickle'\n",
    "\n",
    "                                files_td2 = os.path.join(main_path, 'models-embeddings-friends', file_ext2)\n",
    "\n",
    "                                list_files_tstd= glob.glob(files_td2, recursive=True)\n",
    "\n",
    "                                with (open(list_files_tstd[0], \"rb\")) as openfile:\n",
    "\n",
    "                                    model_tstd = pickle.load(openfile)\n",
    "\n",
    "                                stable_cp_tstd = np.zeros((model_tstd.components_.shape[0], n_voxels))\n",
    "\n",
    "                                for idx in range(model_tstd.components_.shape[0]):\n",
    "\n",
    "                                    img = model_tstd.masker_.inverse_transform(model_tstd.components_[idx,:].todense())\n",
    "\n",
    "                                    stable_cp_tstd[idx, :] = nifti_masker.transform(img)\n",
    "\n",
    "                                del img\n",
    "                                del model_tstd\n",
    "                                gc.collect()\n",
    "\n",
    "                                ## MATCHING COMPONENTS AT THE WITHIN-SUBJECT LEVEL \n",
    "                                tuple_matched_states, scores_states = match_states(stable_cp_td, stable_cp_tstd)\n",
    "                                del stable_cp_tstd\n",
    "                                del stable_cp_td\n",
    "                                gc.collect()\n",
    "\n",
    "                                h5_states.create_dataset('Matched-state-maps', data=np.asarray(tuple_matched_states))\n",
    "\n",
    "                                h5_states.create_dataset('States-reproducibility', data=np.asarray(scores_states))\n",
    "\n",
    "\n",
    "                                del nifti_masker\n",
    "                                del tuple_matched_states\n",
    "\n",
    "                                gc.collect()\n",
    "                                \n",
    "                                print(h5_file.keys())\n",
    "                        #except:\n",
    "                                \n",
    "                        #    continue\n",
    "h5_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
